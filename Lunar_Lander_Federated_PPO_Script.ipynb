{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN7ivH77z8OQPXHTR02UZLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorkobani/Federated-Deep-Reinforcement-Learning/blob/main/Lunar_Lander_Federated_PPO_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL DEPENDENCIES AND IMPORTS"
      ],
      "metadata": {
        "id": "FwybOBMzShG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfzKe6qQSX1k"
      },
      "outputs": [],
      "source": [
        "# Install all necessary libraries\n",
        "!apt-get update && apt-get install -y swig cmake ffmpeg\n",
        "!pip uninstall -y gym\n",
        "!pip install -U flwr[simulation] stable-baselines3[extra] gymnasium[box2d] moviepy\n",
        "\n",
        "import flwr as fl\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE HYPERPARAMETERS"
      ],
      "metadata": {
        "id": "UH9PU4g-TzWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Environment ---\n",
        "ENV_NAME = \"LunarLander-v3\"\n",
        "\n",
        "# --- SB3 PPO Hyperparameters ---\n",
        "ppo_params = {\n",
        "    \"policy\": \"MlpPolicy\",\n",
        "    \"verbose\": 0, # Set to 0 to prevent SB3 from printing its own logs inside the client\n",
        "}\n",
        "\n",
        "# --- Federation Hyperparameters ---\n",
        "NUM_CLIENTS = 5\n",
        "NUM_ROUNDS = 25\n",
        "LOCAL_TIMESTEPS_PER_ROUND = 8192"
      ],
      "metadata": {
        "id": "LNzRt137T3Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE FLOWER CLIENT (wrapping a Stable-Baselines3 Agent)"
      ],
      "metadata": {
        "id": "aHZr8sBVUrI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SB3FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the Flower client.\"\"\"\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.model = PPO(env=self.env, **ppo_params)\n",
        "\n",
        "    def get_parameters(self, config: Dict) -> List[np.ndarray]:\n",
        "        \"\"\"Gets the parameters of the local PPO model's policy network.\"\"\"\n",
        "        return [val.cpu().numpy() for _, val in self.model.policy.state_dict().items()]\n",
        "\n",
        "    def set_parameters(self, parameters: List[np.ndarray]) -> None:\n",
        "        \"\"\"Sets the parameters of the local PPO model's policy network.\"\"\"\n",
        "        params_dict = zip(self.model.policy.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "        self.model.policy.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, parameters: List[np.ndarray], config: Dict) -> Tuple[List[np.ndarray], int, Dict]:\n",
        "        \"\"\"Train the local PPO model.\"\"\"\n",
        "        self.set_parameters(parameters)\n",
        "        self.model.learn(\n",
        "            total_timesteps=int(config[\"local_timesteps\"]),\n",
        "            reset_num_timesteps=False\n",
        "        )\n",
        "        return self.get_parameters(config={}), int(self.model.num_timesteps), {}\n",
        "\n",
        "    def evaluate(self, parameters: List[np.ndarray], config: Dict) -> Tuple[float, int, Dict]:\n",
        "        \"\"\"Evaluate the global PPO model.\"\"\"\n",
        "        self.set_parameters(parameters)\n",
        "        mean_reward, _ = evaluate_policy(\n",
        "            self.model, self.env, n_eval_episodes=10, deterministic=True\n",
        "        )\n",
        "        # FedAvg minimizes loss, so we return negative reward to maximize reward\n",
        "        return -float(mean_reward), 10, {\"avg_reward\": float(mean_reward)}"
      ],
      "metadata": {
        "id": "Y90fkBBXUwiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEDERATED SIMULATION SETUP"
      ],
      "metadata": {
        "id": "iS_aIOHQVEDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def client_fn(cid: str) -> SB3FlowerClient:\n",
        "    return SB3FlowerClient()\n",
        "\n",
        "def fit_config(server_round: int) -> Dict:\n",
        "    \"\"\"Return training configuration dict for each round.\"\"\"\n",
        "    return {\n",
        "        \"server_round\": server_round,\n",
        "        \"local_timesteps\": LOCAL_TIMESTEPS_PER_ROUND,\n",
        "    }\n",
        "\n",
        "def evaluate_metrics_aggregation_fn(metrics: List[Tuple[int, Dict[str, float]]]) -> Dict[str, float]:\n",
        "    \"\"\"Aggregate evaluation results.\"\"\"\n",
        "    if not metrics:\n",
        "        return {}\n",
        "    # Access the history object via a global or other means if needed for round number\n",
        "    # For simplicity, we'll just print based on the list content\n",
        "    avg_rewards = [m[\"avg_reward\"] for _, m in metrics]\n",
        "    mean_reward = sum(avg_rewards) / len(avg_rewards)\n",
        "    # The 'history' object is not available here, so we can't easily get the round number.\n",
        "    # A simple print statement will suffice.\n",
        "    print(f\"Aggregated evaluation results | Average Reward: {mean_reward:.2f}\")\n",
        "    return {\"avg_reward\": mean_reward}\n",
        "\n",
        "# Define the FedAvg strategy\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=1.0,\n",
        "    min_fit_clients=NUM_CLIENTS,\n",
        "    min_evaluate_clients=NUM_CLIENTS,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    on_fit_config_fn=fit_config,\n",
        "    evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
        ")\n",
        "\n",
        "# Start the simulation\n",
        "print(f\"--- Starting Federated PPO Simulation using Stable-Baselines3 ---\")\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "    strategy=strategy,\n",
        "    client_resources={\"num_cpus\": 1},\n",
        ")\n",
        "print(\"\\n--- Federated Learning Final Results ---\")"
      ],
      "metadata": {
        "id": "vKCgOMROVFUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLOT THE RESULTS"
      ],
      "metadata": {
        "id": "q7SK0Mb2VMHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check history.metrics_distributed, not history.metrics_centralized\n",
        "if history.metrics_distributed and \"avg_reward\" in history.metrics_distributed:\n",
        "    # Unpack the list of tuples (round, metric)\n",
        "    rounds, rewards = zip(*history.metrics_distributed[\"avg_reward\"])\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.title(\"Federated PPO with Stable-Baselines3 - Average Reward\")\n",
        "    plt.xlabel(\"Federated Round\")\n",
        "    plt.ylabel(\"Average Reward\")\n",
        "    plt.plot(rounds, rewards)\n",
        "    plt.axhline(y=200, color='r', linestyle='--', label='Success Threshold (200)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No evaluation metrics were recorded.\")"
      ],
      "metadata": {
        "id": "Lu7mInjSVQUE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}