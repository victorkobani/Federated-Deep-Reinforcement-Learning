{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyObAnqvmSIGBBd0zV1D1GQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorkobani/Federated-Deep-Reinforcement-Learning/blob/main/Lunar_Lander_Federated_DQN_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL DEPENDENCIES AND IMPORTS"
      ],
      "metadata": {
        "id": "pt0926ZvXKDW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFIWpeMUW5ev"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get install -y swig cmake ffmpeg\n",
        "!pip uninstall -y gym\n",
        "!pip install -U flwr[simulation] stable-baselines3[extra] gymnasium[box2d] moviepy\n",
        "\n",
        "import flwr as fl\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE HYPERPARAMETERS"
      ],
      "metadata": {
        "id": "Hb7nJVCCXacg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Environment ---\n",
        "ENV_NAME = \"LunarLander-v3\"\n",
        "\n",
        "# --- SB3 DQN Hyperparameters (as per a standard setup) ---\n",
        "DQN_PARAMS = {\n",
        "    \"policy\": \"MlpPolicy\",\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"buffer_size\": 50000,\n",
        "    \"learning_starts\": 1000,\n",
        "    \"batch_size\": 64,\n",
        "    \"gamma\": 0.99,\n",
        "    \"train_freq\": (4, \"step\"),\n",
        "    \"gradient_steps\": 1,\n",
        "    \"target_update_interval\": 250,\n",
        "    \"exploration_fraction\": 0.12,\n",
        "    \"exploration_final_eps\": 0.1,\n",
        "    \"verbose\": 0, # Set to 0 to prevent SB3 from printing its own logs\n",
        "}\n",
        "\n",
        "# --- Federation Hyperparameters ---\n",
        "NUM_CLIENTS = 5\n",
        "NUM_ROUNDS = 25\n",
        "LOCAL_TIMESTEPS_PER_ROUND = 4096 # Adjusted for DQN, which is often more sample-efficient per update cycle"
      ],
      "metadata": {
        "id": "wl2lA85YXfdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE FLOWER CLIENT (wrapping a Stable-Baselines3 DQN Agent)"
      ],
      "metadata": {
        "id": "pHYJB6CmXq03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNFlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        # Instantiate the DQN model with our defined hyperparameters\n",
        "        self.model = DQN(env=self.env, **DQN_PARAMS)\n",
        "\n",
        "    def get_parameters(self, config: Dict) -> List[np.ndarray]:\n",
        "        \"\"\"Gets the parameters of the local DQN model's policy network.\"\"\"\n",
        "        # The parameter extraction logic is identical to PPO\n",
        "        return [val.cpu().numpy() for _, val in self.model.policy.state_dict().items()]\n",
        "\n",
        "    def set_parameters(self, parameters: List[np.ndarray]) -> None:\n",
        "        \"\"\"Sets the parameters of the local DQN model's policy network.\"\"\"\n",
        "        # The parameter setting logic is identical to PPO\n",
        "        params_dict = zip(self.model.policy.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "        self.model.policy.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, parameters: List[np.ndarray], config: Dict) -> Tuple[List[np.ndarray], int, Dict]:\n",
        "        \"\"\"Train the local DQN model.\"\"\"\n",
        "        self.set_parameters(parameters)\n",
        "        # The learn method is called the same way as in PPO\n",
        "        self.model.learn(\n",
        "            total_timesteps=int(config[\"local_timesteps\"]),\n",
        "            reset_num_timesteps=False\n",
        "        )\n",
        "        return self.get_parameters(config={}), int(self.model.num_timesteps), {}\n",
        "\n",
        "    def evaluate(self, parameters: List[np.ndarray], config: Dict) -> Tuple[float, int, Dict]:\n",
        "        \"\"\"Evaluate the global DQN model.\"\"\"\n",
        "        self.set_parameters(parameters)\n",
        "        # The evaluation logic is identical to PPO\n",
        "        mean_reward, _ = evaluate_policy(\n",
        "            self.model, self.env, n_eval_episodes=10, deterministic=True\n",
        "        )\n",
        "        # FedAvg minimizes loss, so we return negative reward to maximize reward\n",
        "        return -float(mean_reward), 10, {\"avg_reward\": float(mean_reward)}"
      ],
      "metadata": {
        "id": "37WSCnLjXwm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEDERATED SIMULATION SETUP"
      ],
      "metadata": {
        "id": "Mr4jX3V5X3hF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def client_fn(cid: str) -> DQNFlowerClient:\n",
        "    \"\"\"Factory function to create a new client.\"\"\"\n",
        "    return DQNFlowerClient()\n",
        "\n",
        "def fit_config(server_round: int) -> Dict:\n",
        "    \"\"\"Return training configuration dict for each round.\"\"\"\n",
        "    # This function is identical to the PPO example\n",
        "    return {\n",
        "        \"server_round\": server_round,\n",
        "        \"local_timesteps\": LOCAL_TIMESTEPS_PER_ROUND,\n",
        "    }\n",
        "\n",
        "def evaluate_metrics_aggregation_fn(metrics: List[Tuple[int, Dict[str, float]]]) -> Dict[str, float]:\n",
        "    \"\"\"Aggregate evaluation results from multiple clients.\"\"\"\n",
        "    # This function is identical to the PPO example\n",
        "    if not metrics:\n",
        "        return {}\n",
        "    # Calculate the mean of the 'avg_reward' metric from all clients\n",
        "    avg_rewards = [m[\"avg_reward\"] for _, m in metrics]\n",
        "    mean_reward = sum(avg_rewards) / len(avg_rewards)\n",
        "    print(f\"Aggregated evaluation results | Average Reward: {mean_reward:.2f}\")\n",
        "    return {\"avg_reward\": mean_reward}\n",
        "\n",
        "# Define the FedAvg strategy, using the same robust setup as the PPO example\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=1.0,\n",
        "    min_fit_clients=NUM_CLIENTS,\n",
        "    min_evaluate_clients=NUM_CLIENTS,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    on_fit_config_fn=fit_config,\n",
        "    evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
        ")\n",
        "\n",
        "# Start the simulation\n",
        "print(f\"--- Starting Federated DQN Simulation using Stable-Baselines3 ---\")\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "    strategy=strategy,\n",
        "    client_resources={\"num_cpus\": 1}, # Assuming CPU-only training\n",
        ")"
      ],
      "metadata": {
        "id": "gFMJgAKuYBBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLOT THE RESULTS"
      ],
      "metadata": {
        "id": "dE9IBD71YGSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Federated Learning Final Results ---\")\n",
        "\n",
        "# The plotting logic is identical to the PPO example\n",
        "if history.metrics_distributed and \"avg_reward\" in history.metrics_distributed:\n",
        "    # Unpack the list of tuples (round, metric)\n",
        "    rounds, rewards = zip(*history.metrics_distributed[\"avg_reward\"])\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.title(\"Federated DQN with Stable-Baselines3 - Average Reward\")\n",
        "    plt.xlabel(\"Federated Round\")\n",
        "    plt.ylabel(\"Average Reward\")\n",
        "    plt.plot(rounds, rewards)\n",
        "    plt.axhline(y=200, color='r', linestyle='--', label='Success Threshold (200)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No distributed evaluation metrics were recorded.\")"
      ],
      "metadata": {
        "id": "9LdF13cWYKgm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}