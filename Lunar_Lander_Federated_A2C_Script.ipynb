{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM+QF/a8e4khJrWQVzHEgMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorkobani/Federated-Deep-Reinforcement-Learning/blob/main/Lunar_Lander_Federated_A2C_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSATLL DEPENDENCIES AND IMPORTS"
      ],
      "metadata": {
        "id": "8Rc-Pi0jYoy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di9VTC_4YeC0"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get install -y swig cmake ffmpeg\n",
        "!pip uninstall -y gym\n",
        "!pip install -U flwr[simulation] stable-baselines3[extra] gymnasium[box2d] moviepy\n",
        "\n",
        "import flwr as fl\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE HYPERPARAMETERS"
      ],
      "metadata": {
        "id": "MxNXZ3VLaBaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Environment ---\n",
        "ENV_NAME = \"LunarLander-v3\"\n",
        "\n",
        "# --- SB3 A2C Hyperparameters ---\n",
        "A2C_PARAMS = {\n",
        "    \"policy\": \"MlpPolicy\",\n",
        "    \"n_steps\": 5,\n",
        "    \"gamma\": 0.99,\n",
        "    \"gae_lambda\": 1.0,\n",
        "    \"ent_coef\": 0.0,\n",
        "    \"vf_coef\": 0.5,\n",
        "    \"max_grad_norm\": 0.5,\n",
        "    \"use_rms_prop\": True,\n",
        "    \"learning_rate\": lambda _: 0.0007, # A2C often uses a constant learning rate\n",
        "    \"verbose\": 0, # Set to 0 to prevent SB3 from printing its own logs\n",
        "}\n",
        "\n",
        "# --- Federation Hyperparameters ---\n",
        "NUM_CLIENTS = 5\n",
        "NUM_ROUNDS = 30 # A2C may need more rounds\n",
        "LOCAL_TIMESTEPS_PER_ROUND = 5000 # On-policy methods like A2C learn from data collected in the current round"
      ],
      "metadata": {
        "id": "HAu8HHIoaGMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE FLOWER CLIENT (wrapping a Stable-Baselines3 A2C Agent)"
      ],
      "metadata": {
        "id": "6Hrixr_naLtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class A2CFlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        # Instantiate the A2C model with our defined hyperparameters\n",
        "        self.model = A2C(env=self.env, **A2C_PARAMS)\n",
        "\n",
        "    def get_parameters(self, config: Dict) -> List[np.ndarray]:\n",
        "        \"\"\"Gets the parameters of the local A2C model's policy network.\"\"\"\n",
        "        # This logic is identical for most SB3 models, including A2C\n",
        "        return [val.cpu().numpy() for _, val in self.model.policy.state_dict().items()]\n",
        "\n",
        "    def set_parameters(self, parameters: List[np.ndarray]) -> None:\n",
        "        \"\"\"Sets the parameters of the local A2C model's policy network.\"\"\"\n",
        "        # This logic is identical for most SB3 models, including A2C\n",
        "        params_dict = zip(self.model.policy.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "        self.model.policy.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, parameters: List[np.ndarray], config: Dict) -> Tuple[List[np.ndarray], int, Dict]:\n",
        "        \"\"\"Train the local A2C model.\"\"\"\n",
        "        self.set_parameters(parameters)\n",
        "        self.model.learn(\n",
        "            total_timesteps=int(config[\"local_timesteps\"]),\n",
        "            reset_num_timesteps=False\n",
        "        )\n",
        "        return self.get_parameters(config={}), int(self.model.num_timesteps), {}\n",
        "\n",
        "    def evaluate(self, parameters: List[np.ndarray], config: Dict) -> Tuple[float, int, Dict]:\n",
        "        \"\"\"Evaluate the global A2C model.\"\"\"\n",
        "        self.set_parameters(parameters)\n",
        "        mean_reward, _ = evaluate_policy(\n",
        "            self.model, self.env, n_eval_episodes=10, deterministic=True\n",
        "        )\n",
        "        # FedAvg minimizes loss, so we return negative reward to maximize reward\n",
        "        return -float(mean_reward), 10, {\"avg_reward\": float(mean_reward)}"
      ],
      "metadata": {
        "id": "L9fQ3BjMaQ8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEDERATED SIMULATION SETUP"
      ],
      "metadata": {
        "id": "2BKp2PxPaW_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def client_fn(cid: str) -> A2CFlowerClient:\n",
        "    \"\"\"Factory function to create a new client.\"\"\"\n",
        "    return A2CFlowerClient()\n",
        "\n",
        "def fit_config(server_round: int) -> Dict:\n",
        "    \"\"\"Return training configuration dict for each round.\"\"\"\n",
        "    return {\n",
        "        \"server_round\": server_round,\n",
        "        \"local_timesteps\": LOCAL_TIMESTEPS_PER_ROUND,\n",
        "    }\n",
        "\n",
        "def evaluate_metrics_aggregation_fn(metrics: List[Tuple[int, Dict[str, float]]]) -> Dict[str, float]:\n",
        "    \"\"\"Aggregate evaluation results from multiple clients.\"\"\"\n",
        "    if not metrics:\n",
        "        return {}\n",
        "    avg_rewards = [m[\"avg_reward\"] for _, m in metrics]\n",
        "    mean_reward = sum(avg_rewards) / len(avg_rewards)\n",
        "    print(f\"Aggregated evaluation results | Average Reward: {mean_reward:.2f}\")\n",
        "    return {\"avg_reward\": mean_reward}\n",
        "\n",
        "# Define the FedAvg strategy, using the same robust setup as the PPO example\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "    fraction_fit=1.0,\n",
        "    fraction_evaluate=1.0,\n",
        "    min_fit_clients=NUM_CLIENTS,\n",
        "    min_evaluate_clients=NUM_CLIENTS,\n",
        "    min_available_clients=NUM_CLIENTS,\n",
        "    on_fit_config_fn=fit_config,\n",
        "    evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
        ")\n",
        "\n",
        "# Start the simulation\n",
        "print(f\"--- Starting Federated A2C Simulation using Stable-Baselines3 ---\")\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "    strategy=strategy,\n",
        "    client_resources={\"num_cpus\": 1}, # Assuming CPU-only training\n",
        ")"
      ],
      "metadata": {
        "id": "cWRbd3nZabfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLOT THE RESULTS"
      ],
      "metadata": {
        "id": "TfhInO7qagp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Federated Learning Final Results ---\")\n",
        "\n",
        "if history.metrics_distributed and \"avg_reward\" in history.metrics_distributed:\n",
        "    # Unpack the list of tuples (round, metric)\n",
        "    rounds, rewards = zip(*history.metrics_distributed[\"avg_reward\"])\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.title(\"Federated A2C with Stable-Baselines3 - Average Reward\")\n",
        "    plt.xlabel(\"Federated Round\")\n",
        "    plt.ylabel(\"Average Reward\")\n",
        "    plt.plot(rounds, rewards)\n",
        "    plt.axhline(y=200, color='r', linestyle='--', label='Success Threshold (200)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No distributed evaluation metrics were recorded.\")"
      ],
      "metadata": {
        "id": "PhvdQNCfam8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}